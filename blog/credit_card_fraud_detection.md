# Credit Card Fraud Detection Using Deep Learning

## Introduction

Standard Machine Learning techniques have been applied to a variety of predictive analytics problems for quite some time. Since 2012 when at Google Prof Andrew Ng made the first breakthrough with Deep Learning techniques to image recognition, the use of Deep Learning with its myriad network architectures has proliferated to numerous domains other than image processing. One such domain is anamoly detection with credit card fraud detction as one special  case.
Worldwide use of credit card is on the rise and so are the frauds associated with credit card transactions. Australians lost almost half a billion dollars in credit card fraud in a single year. Analysis by [consumer comparison website](finder.com.au) found ‘card-not-present’ fraud rose a staggering 76 per cent in the 12 months to June 30, 2018, to 1.8 million dodgy transactions. It is of utmost importance that credit card institutions be able to recognize fraudulent credit card transactions. Traditional approach to a problem like this would be to hard code the pattern recognising rules in rules engine like platforms. However with availability of huge amount of data and exponential growth in compute power, techniques like Deep Learning to learn the patterns from data offer a better alternative. 
In this blog entry, I demonstrate the use of one of the very commonly used yet very powerful neural network architecture to recognise farudulent patterns. 

Any Deep Learning endeavour requires data - lots of data !!!. If your project is a hobby or a pet project, don't get bogged down by the question "Where do I get the data from ?" Websites like [Google Dataset Search](https://toolbox.google.com/datasetsearch) and [Kaggle](kaggle.com), [UCI Machine Learning](https://archive.ics.uci.edu/ml/index.php) are your friends. Real data from different organisations (after anonymisation ) belonging to different domains and potential candidates for application of different ML/DL techniques are made available on these websites. Data publishing organisations publish the data with the intention of AI enthusists building and training ML/DL models  and publishing them which these organisations can use themselves - win-win situation for everybody. 

Data for this blog entry comes from [Kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud/home)

The dataset consists of credit card transactions in September 2013 by european cardholders. This dataset contains transactions carried over a period 2 days and out of 284807 total transactions, 492 are marked as fraudulent. As is clear from this statistics (although it is given in the description of the dataset, the dataset is highly unbalanced : positive class (fraudulent) is only 0.172 %. Although this aspect of the data distribution in this case has already been mentioned, as an AI developer, embarking on modelling a problem like this (classification), studying the data distribution is one of the key exercises to understand your data. The dataset has been anonymised and many attributes (features) have been removed. Features (attributes) have been labelled as V1, V2,..and only numerical data is included (neural nets work on numbers !). When faced with a dataset and dataset having lots of attributes/features, one of the important data preparation steps is to keep only those features which do influence the model. Deciding on which features to keep has evolved into its own area of study called 'feature engineering'. One of the techniques to find out which features are important is called PCA - Principal Component Analysis (topic for another blog !). From the description of the dataset, it is clear that PCA has already been done, so we need not worry about that ! Only features not ransformed with PCA are 'Time' and 'Amount' . 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.

In this blog entry, I show the use of one neural network architecture called 'Autoencoder'. 

## Autoencoders

To understand the autoencoder architecture, reader is assumed to have familiarity with some basics like "supervised learning" vs "unsupervised learning', backpropagation, target values etc. However, I will try to fill in details for readers not so familiar with the concepts.

Autoencoders are artificial neural networks capable of learning efficient representations of the input data. This learned representation is called 'coding' and this learning happens unsupervised. Autoencoders have been used for various purposes in AI. For eaxample one of the most common requirement is that of reducing the dimension of input data.  The coding that autoencoder learns has much smaller dimension than the input. This architectural aspect of autoencoders make them very suitable for dimensionality reduction. Other use cases include unsupervised pretraining of deep neural networks, randomly generating data that looks very similar to the training data, which is called a generative model. An autoencoder can be trained on pictures of faces and then this trained network will be able to generate new faces.

The way autoencoder learns is by simply copying the input to output i.e it learns an Identity function. This might sound very simple, but by constraining the network in various ways, this seemingly simple task can be made very difficult and force autoencoder to learn interesting representations. Some of the ways an autoencoder could constrained, includes limiting the size of internal representation (i.e number of input nodes much larger than nodes in hidden layer), adding noise to the input data

To understand the working of autoencoder, I draw an anlogy with the way the mind of a chess player works. An expert chess player can memorise the positions of all the pieces by just looking at the board for 5 seconds. The magic lies in the fact that he/she can do this when the pieces are lying according to some well known pattern, which he has already experienced before ! Similalry, an autoencoder looks at the input and converts them to an efficient internal representation

Autoencoder  neural network is an example of 'Unsupervised Learning'. Unsupervised learning involves building model from data with no help provided by the data in terms of any labelling. Some synonyms for 'Labels' are 'Target' or 'Target Values'. A typical "Supervised Learning" will involve having dataset consisting of example data some of which has been labelled in some way and then the data is passed through the network with network outputting what it thinks what the data is ! e.g the real data (label) may be 'Cat' but the network thinks it is 'Rabbit' ! So obviously there is difference between what the real label is and what the network predicted. To fix this (or to minimise) the error, error is 'backpropagated' through the different layers apportioning the error to different nodes (obviously no single node is culprit, all nodes must have contributed to the final error). All the nodes are thus updated (actually their weights) and this cycle repeats till we are able to reach some predecided level of error, each step improving the network. Backpropagation is "the most important" technique for Deep Learning and needs a separate blog entry of its own. However, it is sufficient to know that it is a mechanism to propagate back the error with each node getting its own portion of error. In "Unsupervised Learning" there is no labelled data !!! So what does error mean in this context ? Unless there is a target value, there is no error and nothing to backpropagate and nothing to learn !! For autoencoder network we set the target values equal to input values ! 

![](Autoencoder.png)


In the above diagram we have unlabelled input data as real numbers(x<sub>1</sub>, x<sub>2</sub> ...) We set the target values equal to inputs. The autoencoder tries to learn an Identity function, so as out put Xhat that is similar to x. 




